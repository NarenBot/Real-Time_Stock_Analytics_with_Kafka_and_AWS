{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd24f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "import time\n",
    "import json\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12b0edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "      <th>unwanted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.879006</td>\n",
       "      <td>6.596165e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-02</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.213013</td>\n",
       "      <td>6.452109e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-05</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.811987</td>\n",
       "      <td>6.514745e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-06</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.906987</td>\n",
       "      <td>6.676539e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-07</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.923013</td>\n",
       "      <td>6.796971e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Index        Date         Open         High          Low        Close  \\\n",
       "0   HSI  1986-12-31  2568.300049  2568.300049  2568.300049  2568.300049   \n",
       "1   HSI  1987-01-02  2540.100098  2540.100098  2540.100098  2540.100098   \n",
       "2   HSI  1987-01-05  2552.399902  2552.399902  2552.399902  2552.399902   \n",
       "3   HSI  1987-01-06  2583.899902  2583.899902  2583.899902  2583.899902   \n",
       "4   HSI  1987-01-07  2607.100098  2607.100098  2607.100098  2607.100098   \n",
       "\n",
       "     Adj Close  Volume    CloseUSD      unwanted  \n",
       "0  2568.300049     0.0  333.879006  6.596165e+06  \n",
       "1  2540.100098     0.0  330.213013  6.452109e+06  \n",
       "2  2552.399902     0.0  331.811987  6.514745e+06  \n",
       "3  2583.899902     0.0  335.906987  6.676539e+06  \n",
       "4  2607.100098     0.0  338.923013  6.796971e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"stock_analysis2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9b9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer('test_topic', \n",
    "                         bootstrap_servers=['54.235.13.11:9092'], \n",
    "                         value_deserializer=lambda x: json.loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c023dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConsumerRecord(topic='test_topic', partition=0, offset=25, timestamp=1723106679391, timestamp_type=0, key=None, value={'Index': 'N225', 'Date': '1995-08-08', 'Open': 16645.24023, 'High': 16853.5, 'Low': 16512.17969, 'Close': 16838.9707, 'Adj Close': 16838.9707, 'Volume': 0.0, 'CloseUSD': 168.38970700000002, 'unwanted': 280288712.52743125}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\n",
      "ConsumerRecord(topic='test_topic', partition=0, offset=26, timestamp=1723106681418, timestamp_type=0, key=None, value={'Index': 'HSI', 'Date': '2008-06-27', 'Open': 21901.26953, 'High': 22201.4707, 'Low': 21773.66992, 'Close': 22042.34961, 'Adj Close': 22042.34961, 'Volume': 2215868800.0, 'CloseUSD': 2865.5054493, 'unwanted': 482755439.8831005}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=228, serialized_header_size=-1)\n",
      "ConsumerRecord(topic='test_topic', partition=0, offset=27, timestamp=1723106683433, timestamp_type=0, key=None, value={'Index': 'N225', 'Date': '1991-12-11', 'Open': 21900.7207, 'High': 21900.7207, 'Low': 21123.90039, 'Close': 21502.90039, 'Adj Close': 21502.90039, 'Volume': 0.0, 'CloseUSD': 215.0290039, 'unwanted': 470929015.6813111}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=218, serialized_header_size=-1)\n",
      "ConsumerRecord(topic='test_topic', partition=0, offset=28, timestamp=1723106685449, timestamp_type=0, key=None, value={'Index': 'N225', 'Date': '1969-07-01', 'Open': 1969.670044, 'High': 1969.670044, 'Low': 1969.670044, 'Close': 1969.670044, 'Adj Close': 1969.670044, 'Volume': 0.0, 'CloseUSD': 19.69670044, 'unwanted': 3879600.082230962}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\n",
      "ConsumerRecord(topic='test_topic', partition=0, offset=29, timestamp=1723106687475, timestamp_type=0, key=None, value={'Index': 'GSPTSE', 'Date': '2005-04-01', 'Open': 9675.299805, 'High': 9679.0, 'Low': 9612.400391, 'Close': 9638.599609, 'Adj Close': 9638.599609, 'Volume': 13353770000.0, 'CloseUSD': 8000.03767547, 'unwanted': 93256340.9174308}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=228, serialized_header_size=-1)\n",
      "ConsumerRecord(topic='test_topic', partition=0, offset=30, timestamp=1723106689508, timestamp_type=0, key=None, value={'Index': 'HSI', 'Date': '2006-09-04', 'Open': 17504.33008, 'High': 17538.58984, 'Low': 17476.59961, 'Close': 17513.88086, 'Adj Close': 17513.88086, 'Volume': 307024000.0, 'CloseUSD': 2276.8045118, 'unwanted': 306568751.5552343}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=228, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in consumer:\n",
    "    print(i)\n",
    "    if count >=5:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "# Output:\n",
    "# ConsumerRecord(topic='kafkatest', partition=0, offset=109, timestamp=1723015251460, timestamp_type=0, key=None, value={'Index': '000001.SS', 'Date': '2004-04-14', 'Open': 1714.526001, 'High': 1716.819946, 'Low': 1678.33606, 'Close': 1697.156006, 'Adj Close': 1697.156006, 'Volume': 19600.0, 'CloseUSD': 271.54496096}, headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=198, serialized_header_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb1498f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9beed726",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(consumer):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m s3\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://kafka-stock-analytics/landing-zone/stock_market_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(i\u001b[38;5;241m.\u001b[39mvalue, file)\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\fsspec\\spec.py:2035\u001b[0m, in \u001b[0;36mAbstractBufferedFile.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforced:\n\u001b[1;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39minvalidate_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\fsspec\\spec.py:1899\u001b[0m, in \u001b[0;36mAbstractBufferedFile.flush\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m   1896\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\s3fs\\core.py:2371\u001b[0m, in \u001b[0;36mS3File._upload_chunk\u001b[1;34m(self, final)\u001b[0m\n\u001b[0;32m   2368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\u001b[38;5;241m.\u001b[39mappend(part_header)\n\u001b[0;32m   2370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mand\u001b[39;00m final:\n\u001b[1;32m-> 2371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\s3fs\\core.py:2389\u001b[0m, in \u001b[0;36mS3File.commit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macl:\n\u001b[0;32m   2388\u001b[0m         kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macl\n\u001b[1;32m-> 2389\u001b[0m     write_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mput_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\s3fs\\core.py:2234\u001b[0m, in \u001b[0;36mS3File._call_s3\u001b[1;34m(self, method, *kwarglist, **kwargs)\u001b[0m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_s3\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;241m*\u001b[39mkwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwarglist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\fsspec\\asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\site-packages\\fsspec\\asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Naren\\iNeuron\\Internship_Projects\\BigData_Projects\\Real-Time_Stock_Analytics_with_Kafka_and_AWS\\venvKafka\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for count, i in enumerate(consumer):\n",
    "    with s3.open(f\"s3://kafka-stock-analytics/landing-zone/stock_market_{count}.json\", \"w\") as file:\n",
    "        json.dump(i.value, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2973ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190b16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9e32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc041c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6cb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
